# Deploy-an-RL-policy-on-the-Unitree-Go2-robot
This repository provides a framework for low-level control of a legged robot (Unitree Go2), using ROS 2 as the communication middleware. The MuJoCo simulator is used to validate the control policy in simulation. Once the policy performs well in MuJoCo, you can deploy it on the real robot by simply setting the ROS parameter is_simulation to false. Also a base velocity estimator using extended Karman Filter is provided to estimate the velocity of base. 

## Environment
- **Ubuntu**: 20.04
- **ROS 2**: Foxy
- **MuJoCo**: 3.2.3
- **Python**: 3.8.20
- **Pinocchio**: 3.4.0

## Access Robot Sensor Data via ROS 2
Refer to the official Unitree ROS 2 repository for setup and examples:
[unitree ros2](https://github.com/unitreerobotics/unitree_ros2)

### Potential Build Issue
When building the ROS 2 packages, you may encounter the following error:
```bash
ModuleNotFoundError: No module named 'unitree_go.unitree_go_s__rosidl_typesupport_c’
```
or 
``` bash
rosidl_generator_py.import_type_support_impl.UnsupportedTypeSupport: Could not import 'rosidl_typesupport_c' for package 'unitree_go’
```
This issue occurs because **ROS 2 Foxy supports only Python 3.8** for generating C-based type support modules. If a different Python version is used, the build process may still succeed, but the runtime will **fail to locate the generated files**, resulting in import errors when executing ROS 2 commands.

### Solution
Manually set your Python version to **3.8** when building the workspace. For example:
```bash
export PYTHON_EXECUTABLE=/usr/bin/python3.8
colcon build --symlink-install
```
Make sure Python 3.8 is installed and available at the specified path.

## Simulation
After successfully building this workspace, you can launch the simulation in mujoco with the following commands:
```bash
source install/setup.bash
ros2 run deploy_RL_policy mujoco_simulator.py
``` 
Here is a screenshot of the simulation scene:
<p align="center">
  <img src="./resources/images/mujoco.png" alt="MuJoCo Simulation Scene" width="500"/>
</p>

## Control Logic
The robot's behavior is controlled by a 3-state state machine, including:
* **Laying Down**
* **Standing Up**
* **Executing RL Policy**

### State Transition (XBox Controller)
* **Initial State:** Robot automatically enters "Laying Down" state
* **B Button:** Transitions from "Laying Down" → "Standing Up"
* **A Button:** Transitions from "Standing Up" → "Laying Down"
* **LB + RB Simultaneously:** While standing, executes RL Policy (remains in standing state)

### Notes:
* The "Executing RL Policy" state is considered a special case of the "Standing Up" state
* Controller inputs are only processed when the robot is in the appropriate state for that transition

### Launch Control Nodes

Run the following commands in separate terminals to activate the control system:

```bash
# Terminal 1: XBox Controller Interface
ros2 run joy joy_node

# Terminal 2: State Machine Controller
ros2 run deploy_RL_policy low_level_control --ros-args -p is_simulation:=true # true: simulation  false: real robot

# Terminal 3: Reinforcement Learning Policy
ros2 run deploy_RL_policy RL_policy.py --is_simulation True  # or False
```
Node Description:
1. joy_node
    * Interfaces with XBox controller hardware
    * Publishes controller input to /joy topic
2. low_level_control
    * Implements the 3-state machine (Laying Down/Standing Up/RL Policy)
    * Handles state transitions based on controller input
    * Sends lowcmd to simulator or the real robot
3. ​​RL_policy.py​​:
    * Executes reinforcement learning policy
    * Activated only in "Executing RL Policy" state (LB+RB pressed while standing)

## Implementing Custom Policies

To use your own reinforcement learning policy with the system:

1. **Modify Policy Path**  
   Edit the policy file path in `RL_policy.py` to point to your custom policy.

2. **Data Sequence Considerations**  
   - The Unitree robot uses a specific joint order that may differ from your training environment
   - Verify your policy's output sequence matches the robot's expected input order

3. **Safety Recommendations**  
   ```diff
   + Always test new policies in simulation first
   - Avoid deploying untested policies directly to hardware
   ```
You can refer to the [official documentation](https://support.unitree.com/home/en/developer/Basic_services) to check the correct joint order.

## Base Velocity Estimator
If your policy requires the base velocity as part of the observation and it's not available from onboard sensors, you can use the **Base Velocity Estimator** to estimate it.

It's implemented as an **Extended Kalman Filter (EKF)** with a measurement model and a system model. The measurement is computed from kinematic equations using [Pinocchio](https://github.com/stack-of-tasks/pinocchio). Here's a rough overview of the theory behind it:
### Asumptions
- When the foot is in contact with the ground and there's no slippage:  
  $$\mathbf{v}_{\text{foot}}^w = \mathbf{0}$$
- Known from sensors:
    - Joint angels $\boldsymbol{\theta}$ and velocities $\dot{\boldsymbol{\theta}}$ (from motor encoders)
    - Angular velocity $\boldsymbol{\omega}^b$ (from IMU)
    - Foot position relative to the base $\mathbf{r}_{\text{foot}}^b$ (computed by forward kinematics)

### Equations  
1. The velocity of the foot is composed of:
- Velocity due to base motion:
     $$
     \mathbf{v}_0 = \mathbf{v}_{\text{base}}^w + \boldsymbol{\omega}^w \times \mathbf{r}_{\text{foot}}^w
     $$
   - Velocity from joint motion:
     $$
     \mathbf{v}_1 = \mathbf{J}(\boldsymbol{\theta}) \dot{\boldsymbol{\theta}}
     $$
   - So total foot velocity in world frame:
     $$
     \mathbf{v}_{\text{foot}}^w = \mathbf{v}_{\text{base}}^w + \boldsymbol{\omega}^w \times \mathbf{r}_{\text{foot}}^w + \mathbf{R}_{b}^w \mathbf{J}(\boldsymbol{\theta}) \dot{\boldsymbol{\theta}}
     $$
     
2. if the foot is in contact with ground:
   $$\mathbf{v}_{\text{foot}}^w = \mathbf{0}$$ 
   then:
$$\mathbf{v}_{\text{base}}^w = - \boldsymbol{\omega}^w \times \mathbf{r}_{\text{foot}}^w - \mathbf{R}_{b}^w \mathbf{J}(\boldsymbol{\theta}) \dot{\boldsymbol{\theta}}$$

3. Transform to base frame to get what we need:
   $$
   \mathbf{v}_{\text{base}}^b = \mathbf{R}_{w}^b \mathbf{v}_{\text{base}}^w
   $$
   which gives:
   $$
   \mathbf{v}_{\text{base}}^b = - \boldsymbol{\omega}^b \times \mathbf{r}_{\text{foot}}^b - \mathbf{J}(\boldsymbol{\theta}) \dot{\boldsymbol{\theta}}
   $$
 ### System Model
The EKF system model predicts base velocity by integrating acceleration from the IMU, which unfortunately tends to drift over time:
$$
\mathbf{v}_{\text{system}} = \sum_{t=0}^T a_t \Delta t
$$
This system model and the measurement model are fused in the EKF to get a more stable and accurate base velocity estimate.

### Using Neural Network

It's also possible to estimate the base velocity using a **deep neural network**. By feeding the network with the past few observations as input and training it to predict the current base velocity, you can often achieve even better accuracy than model-based methods.

No need for complex architectures — even a simple **MLP** (Multi-Layer Perceptron) can converge quickly and perform well!
